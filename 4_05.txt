/opt/conda/lib/python3.12/site-packages/ignite/handlers/checkpoint.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import ZeroRedundancyOptimizer
2025-08-04 08:06:25,365 ignite.distributed.launcher.Parallel INFO: - Run '<function training at 0x7f4982816ac0>' in 1 processes
2025-08-04 08:06:25,385 ALCD_CLOUD INFO: Train ags_tiny_unet_50k on CIFAR10
2025-08-04 08:06:25,385 ALCD_CLOUD INFO: - PyTorch version: 2.4.1+cu121
2025-08-04 08:06:25,385 ALCD_CLOUD INFO: - Ignite version: 0.5.1
2025-08-04 08:06:25,404 ALCD_CLOUD INFO: - GPU Device: NVIDIA GeForce RTX 3090
2025-08-04 08:06:25,405 ALCD_CLOUD INFO: - CUDA version: 12.1
2025-08-04 08:06:25,406 ALCD_CLOUD INFO: - CUDNN version: 90100
2025-08-04 08:06:25,406 ALCD_CLOUD INFO: 

2025-08-04 08:06:25,406 ALCD_CLOUD INFO: Configuration:
2025-08-04 08:06:25,406 ALCD_CLOUD INFO: 	seed: 12345
2025-08-04 08:06:25,406 ALCD_CLOUD INFO: 	data_path: /pvc
2025-08-04 08:06:25,406 ALCD_CLOUD INFO: 	csv_paths: {'train': '/pvc/train.csv', 'test': '/pvc/valid.csv'}
2025-08-04 08:06:25,406 ALCD_CLOUD INFO: 	output_path: ./output-alcd-cloud/
2025-08-04 08:06:25,406 ALCD_CLOUD INFO: 	input_size: (512, 512)
2025-08-04 08:06:25,406 ALCD_CLOUD INFO: 	img_mean: [0.0, 0, 0.0]
2025-08-04 08:06:25,406 ALCD_CLOUD INFO: 	img_rescale: [8657.0, 8657.0, 8657.0]
2025-08-04 08:06:25,406 ALCD_CLOUD INFO: 	model: ags_tiny_unet_50k
2025-08-04 08:06:25,406 ALCD_CLOUD INFO: 	quant_config: None
2025-08-04 08:06:25,406 ALCD_CLOUD INFO: 	class_weights: [0.1, 0.9]
2025-08-04 08:06:25,406 ALCD_CLOUD INFO: 	batch_size: 28
2025-08-04 08:06:25,406 ALCD_CLOUD INFO: 	weight_decay: 0.0002
2025-08-04 08:06:25,406 ALCD_CLOUD INFO: 	num_workers: 8
2025-08-04 08:06:25,406 ALCD_CLOUD INFO: 	num_epochs: 100
2025-08-04 08:06:25,406 ALCD_CLOUD INFO: 	learning_rate: 0.001
2025-08-04 08:06:25,406 ALCD_CLOUD INFO: 	num_warmup_epochs: 5
2025-08-04 08:06:25,406 ALCD_CLOUD INFO: 	validate_every: 3
2025-08-04 08:06:25,406 ALCD_CLOUD INFO: 	checkpoint_every: 1000
2025-08-04 08:06:25,406 ALCD_CLOUD INFO: 	backend: None
2025-08-04 08:06:25,406 ALCD_CLOUD INFO: 	resume_from: None
2025-08-04 08:06:25,406 ALCD_CLOUD INFO: 	log_every_iters: 5
2025-08-04 08:06:25,406 ALCD_CLOUD INFO: 	nproc_per_node: None
2025-08-04 08:06:25,406 ALCD_CLOUD INFO: 	stop_iteration: None
2025-08-04 08:06:25,406 ALCD_CLOUD INFO: 	with_amp: False
2025-08-04 08:06:25,406 ALCD_CLOUD INFO: 

2025-08-04 08:06:25,406 ALCD_CLOUD INFO: Output path: output-alcd-cloud/ags_tiny_unet_50k_backend-None-1_20250804-080625
2025-08-04 08:06:25,447 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset '<utils.utils.ALCDDat': 
	{'batch_size': 28, 'num_workers': 8, 'shuffle': True, 'drop_last': True, 'pin_memory': True}
/opt/conda/lib/python3.12/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2025-08-04 08:06:25,448 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset '<utils.utils.ALCDDat': 
	{'batch_size': 56, 'num_workers': 8, 'shuffle': False, 'pin_memory': True}
/home/jovyan/./SCRIPTS/train.py:366: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler(enabled=with_amp)
2025-08-04 08:06:25,689 ALCD_CLOUD INFO: Engine run starting with max_epochs=100.
/opt/conda/lib/python3.12/site-packages/rasterio/__init__.py:356: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.
  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)
/opt/conda/lib/python3.12/site-packages/rasterio/__init__.py:356: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.
  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)
/opt/conda/lib/python3.12/site-packages/rasterio/__init__.py:356: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.
  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)
/opt/conda/lib/python3.12/site-packages/rasterio/__init__.py:356: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.
  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)
/opt/conda/lib/python3.12/site-packages/rasterio/__init__.py:356: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.
  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)
/opt/conda/lib/python3.12/site-packages/rasterio/__init__.py:356: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.
  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)
/opt/conda/lib/python3.12/site-packages/rasterio/__init__.py:356: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.
  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)
/opt/conda/lib/python3.12/site-packages/rasterio/__init__.py:356: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.
  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)
2025-08-04 08:06:29,801 ALCD_CLOUD ERROR: Current run is terminating due to exception: CUDA out of memory. Tried to allocate 1.31 GiB. GPU 0 has a total capacity of 23.57 GiB of which 964.75 MiB is free. Process 129 has 13.90 GiB memory in use. Including non-PyTorch memory, this process has 8.71 GiB memory in use. Of the allocated memory 8.37 GiB is allocated by PyTorch, and 42.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-04 08:06:34,814 ALCD_CLOUD ERROR: Engine run is terminating due to exception: CUDA out of memory. Tried to allocate 1.31 GiB. GPU 0 has a total capacity of 23.57 GiB of which 964.75 MiB is free. Process 129 has 13.90 GiB memory in use. Including non-PyTorch memory, this process has 8.71 GiB memory in use. Of the allocated memory 8.37 GiB is allocated by PyTorch, and 42.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-04 08:06:34,814 ALCD_CLOUD ERROR: 
Traceback (most recent call last):
  File "/home/jovyan/./SCRIPTS/train.py", line 173, in training
    trainer.run(train_loader, max_epochs=config["num_epochs"])
  File "/opt/conda/lib/python3.12/site-packages/ignite/engine/engine.py", line 889, in run
    return self._internal_run()
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/site-packages/ignite/engine/engine.py", line 932, in _internal_run
    return next(self._internal_run_generator)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/site-packages/ignite/engine/engine.py", line 990, in _internal_run_as_gen
    self._handle_exception(e)
  File "/opt/conda/lib/python3.12/site-packages/ignite/engine/engine.py", line 644, in _handle_exception
    raise e
  File "/opt/conda/lib/python3.12/site-packages/ignite/engine/engine.py", line 956, in _internal_run_as_gen
    epoch_time_taken += yield from self._run_once_on_dataset_as_gen()
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/site-packages/ignite/engine/engine.py", line 1096, in _run_once_on_dataset_as_gen
    self._handle_exception(e)
  File "/opt/conda/lib/python3.12/site-packages/ignite/engine/engine.py", line 644, in _handle_exception
    raise e
  File "/opt/conda/lib/python3.12/site-packages/ignite/engine/engine.py", line 1077, in _run_once_on_dataset_as_gen
    self.state.output = self._process_function(self, self.state.batch)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jovyan/./SCRIPTS/train.py", line 392, in train_step
    y_pred = model(x)
             ^^^^^^^^
  File "/opt/conda/lib/python3.12/site-packages/torch/fx/graph_module.py", line 738, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/site-packages/torch/fx/graph_module.py", line 316, in __call__
    raise e
  File "/opt/conda/lib/python3.12/site-packages/torch/fx/graph_module.py", line 303, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<eval_with_key>.0", line 78, in forward
    decoder_cat_layers_3_concat = getattr(self, "decoder/cat_layers/3/Concat")(decoder_decoder_blocks_3_up_sample_layer_resize, encoder_encoder_blocks_0_encoder_block_5_leaky_relu);  decoder_decoder_blocks_3_up_sample_layer_resize = encoder_encoder_blocks_0_encoder_block_5_leaky_relu = None
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/site-packages/onnx2torch/node_converters/concat.py", line 22, in forward
    return torch.cat(input_tensors, self.axis)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.31 GiB. GPU 0 has a total capacity of 23.57 GiB of which 964.75 MiB is free. Process 129 has 13.90 GiB memory in use. Including non-PyTorch memory, this process has 8.71 GiB memory in use. Of the allocated memory 8.37 GiB is allocated by PyTorch, and 42.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/jovyan/./SCRIPTS/train.py", line 487, in <module>
    fire.Fire({"run": run})
  File "/opt/conda/lib/python3.12/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jovyan/./SCRIPTS/train.py", line 260, in run
    parallel.run(training, config)
  File "/opt/conda/lib/python3.12/site-packages/ignite/distributed/launcher.py", line 316, in run
    func(local_rank, *args, **kwargs)
  File "/home/jovyan/./SCRIPTS/train.py", line 176, in training
    raise e
  File "/home/jovyan/./SCRIPTS/train.py", line 173, in training
    trainer.run(train_loader, max_epochs=config["num_epochs"])
  File "/opt/conda/lib/python3.12/site-packages/ignite/engine/engine.py", line 889, in run
    return self._internal_run()
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/site-packages/ignite/engine/engine.py", line 932, in _internal_run
    return next(self._internal_run_generator)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/site-packages/ignite/engine/engine.py", line 990, in _internal_run_as_gen
    self._handle_exception(e)
  File "/opt/conda/lib/python3.12/site-packages/ignite/engine/engine.py", line 644, in _handle_exception
    raise e
  File "/opt/conda/lib/python3.12/site-packages/ignite/engine/engine.py", line 956, in _internal_run_as_gen
    epoch_time_taken += yield from self._run_once_on_dataset_as_gen()
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/site-packages/ignite/engine/engine.py", line 1096, in _run_once_on_dataset_as_gen
    self._handle_exception(e)
  File "/opt/conda/lib/python3.12/site-packages/ignite/engine/engine.py", line 644, in _handle_exception
    raise e
  File "/opt/conda/lib/python3.12/site-packages/ignite/engine/engine.py", line 1077, in _run_once_on_dataset_as_gen
    self.state.output = self._process_function(self, self.state.batch)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jovyan/./SCRIPTS/train.py", line 392, in train_step
    y_pred = model(x)
             ^^^^^^^^
  File "/opt/conda/lib/python3.12/site-packages/torch/fx/graph_module.py", line 738, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/site-packages/torch/fx/graph_module.py", line 316, in __call__
    raise e
  File "/opt/conda/lib/python3.12/site-packages/torch/fx/graph_module.py", line 303, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<eval_with_key>.0", line 78, in forward
  File "/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.12/site-packages/onnx2torch/node_converters/concat.py", line 22, in forward
    return torch.cat(input_tensors, self.axis)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.31 GiB. GPU 0 has a total capacity of 23.57 GiB of which 964.75 MiB is free. Process 129 has 13.90 GiB memory in use. Including non-PyTorch memory, this process has 8.71 GiB memory in use. Of the allocated memory 8.37 GiB is allocated by PyTorch, and 42.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
